{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.155/6 Challenge Problem 2\n",
    "<div style=\"font-size: small;\">\n",
    "License Terms:  \n",
    "These Python demos are licensed under a <a href=\"https://creativecommons.org/licenses/by-nc-nd/4.0/\">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>. They are intended for educational use only in Class 2.155/2.156: AI and ML for Engineering Design at MIT. You may not share or distribute them publicly, use them for commercial purposes, or provide them to industry or other entities without permission from the instructor (faez@mit.edu).\n",
    "</div>\n",
    "\n",
    "<font size=\"1\">\n",
    "  Pixel Art by J. Shung. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- Feature engineering (proximity)\n",
    "- Design new grids using GAs and rules of observations how advisors work\n",
    "- Select better grids for diversity (not just off top minimum scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_public import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel, RBF\n",
    "from sklearn.base import clone\n",
    "\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# This was needed to prevent several of the models from locking up when loading from autogluon saves\n",
    "os.environ.update({\"OMP_NUM_THREADS\":\"1\",\"MKL_NUM_THREADS\":\"1\",\"OPENBLAS_NUM_THREADS\":\"1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = load_grids()\n",
    "ratings = np.load(\"datasets/scores.npy\")\n",
    "ratings_df = pd.DataFrame(ratings, columns = [\"Wellness\", \"Tax\", \"Transportation\", \"Business\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO generate new grids with genetic algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Regressors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import r2_score\n",
    "# import time\n",
    "# def plot_and_r2(preds_train, preds_test, ratings_train, ratings_test, advisor): \n",
    "#     #Calculates \n",
    "#     plt.figure(figsize=(6,3))\n",
    "#     plt.scatter(ratings_train, preds_train, label='Train Set Preds', s=3, c = \"#BBBBBB\") #train set in gray\n",
    "#     plt.scatter(ratings_test, preds_test, label='Test Set Preds', s=5, c = \"#DC267F\") #test set in magenta\n",
    "#     plt.plot([0,1], [0,1], label=\"Target\", linewidth=3, c=\"k\") #target line in black\n",
    "\n",
    "#     #Set axis labels and title\n",
    "#     plt.xlabel(\"Actual Rating\")\n",
    "#     plt.ylabel(\"Predicted Rating\")\n",
    "#     plt.title(f\"Advisor {advisor} Predictions\")\n",
    "\n",
    "#     #Turn off top and right spines\n",
    "#     ax = plt.gca()\n",
    "#     ax.spines['right'].set_visible(False)\n",
    "#     ax.spines['top'].set_visible(False)\n",
    "\n",
    "#     plt.legend() #Display legend\n",
    "#     plt.show() #Show plot\n",
    "\n",
    "#     #Calculate R2 score for train and test sets\n",
    "#     print(f\"Advisor {advisor} Train Set R2 score: {r2_score(ratings_train, preds_train)}\") \n",
    "#     print(f\"Advisor {advisor} Test Set R2 score: {r2_score(ratings_test, preds_test)}\")\n",
    "\n",
    "# def append_district_counts(grids): #performs the feature engineering to add district counts\n",
    "#     grids_flat = grids.reshape(-1, 49) #first flatten the grids\n",
    "\n",
    "#     counts = [np.sum(grids_flat==0, axis=1),\n",
    "#               np.sum(grids_flat==1, axis=1),\n",
    "#               np.sum(grids_flat==2, axis=1),\n",
    "#               np.sum(grids_flat==3, axis=1),\n",
    "#               np.sum(grids_flat==4, axis=1)] #list of 5 length n_grids arrays containing counts of each district\n",
    "#     features = np.stack(counts).T #stack and transpose counts to get n_grids x 5 array\n",
    "#     out = np.hstack([grids_flat, features]) #stack the features horizontally with the flattened grids\n",
    "#     return out.astype(np.float32)  # ensure float32 dtype\n",
    "\n",
    "# def FE_split_train_eval(grids, FE_fn, advisor, ratings_subset): #feature engineering, split, train, evaluate\n",
    "#     # features -> DataFrame with numeric dtype\n",
    "#     grids_fa = pd.DataFrame(FE_fn(grids)).astype(np.float32)\n",
    "#     grids_train, grids_test, ratings_train, ratings_test = train_test_split(grids_fa, ratings_subset) #split\n",
    "#     ratings_train = np.array(ratings_train, dtype=np.float32)\n",
    "#     ratings_test  = np.array(ratings_test, dtype=np.float32)\n",
    "\n",
    "#     all_train = grids_train.copy().astype(np.float32)\n",
    "#     all_train[\"label\"] = ratings_train\n",
    "\n",
    "#     predictor = TabularPredictor(label='label',\n",
    "#                                  problem_type='regression',\n",
    "#                                  verbosity=2,\n",
    "#                                  eval_metric='r2')\n",
    "#     predictor.fit(all_train, presets='best', time_limit=180)\n",
    "#     predictor.delete_models(models_to_keep='best')\n",
    "\n",
    "#     preds_train = predictor.predict(grids_train.astype(np.float32))\n",
    "#     preds_test  = predictor.predict(grids_test.astype(np.float32))\n",
    "\n",
    "#     # convert predictions to float32 numpy for downstream numeric ops\n",
    "#     preds_train = np.array(preds_train, dtype=np.float32)\n",
    "#     preds_test  = np.array(preds_test, dtype=np.float32)\n",
    "\n",
    "#     plot_and_r2(preds_train, preds_test, ratings_train, ratings_test, advisor) #plot and calculate R2\n",
    "#     return predictor\n",
    "\n",
    "# def merge_predictions(grids, ratings, FE_fn, predictor): #Combine predictions and real ratings\n",
    "#     full_dataset_FA = pd.DataFrame(FE_fn(grids)).astype(np.float32) #feature engineering on full dataset\n",
    "#     predictions = predictor.predict(full_dataset_FA) #predict on full dataset\n",
    "\n",
    "#     predictions = np.array(predictions, dtype=np.float32)\n",
    "#     ratings = np.array(ratings, dtype=np.float32)\n",
    "\n",
    "#     mask = ~np.isnan(ratings) # boolean mask of rated entries\n",
    "#     if mask.any():\n",
    "#         predictions[mask] = ratings[mask] #replace the predictions with the actual ratings where available\n",
    "#     return predictions\n",
    "\n",
    "# def fit_plot_predict(grids, ratings, FE_fn, advisor):\n",
    "#     grids_subset, ratings_subset = select_rated_subset(grids, ratings[:,advisor]) #gets subset of the dataset rated by advisor 0\n",
    "#     predictor = FE_split_train_eval(grids_subset, FE_fn, advisor, ratings_subset) #feature engineering, split, train, evaluate\n",
    "#     predictions = merge_predictions(grids, ratings[:,advisor], FE_fn, predictor) #merge predictions with actual ratings\n",
    "#     return predictions\n",
    "\n",
    "# def advisor_train(grids, ratings, advisor):\n",
    "#     fit_plot_predict(grids, ratings, FE_fns[advisor], advisor)\n",
    "\n",
    "\n",
    "# def advisor_predict(grids, ratings, advisor):\n",
    "#     print(f\"Predicting Advisor {advisor}...\")\n",
    "#     models = [\"ag-20251021_010555\",\n",
    "#               \"ag-20251020_233146\",\n",
    "#               \"ag-20251020_235104\",\n",
    "#               \"ag-20251020_235823\",]\n",
    "    \n",
    "#     t1 = time.time()\n",
    "#     predictor = TabularPredictor.load(\"AutogluonModels/\"+models[advisor])\n",
    "#     predictions = merge_predictions(grids, ratings[:,advisor], FE_fns[advisor], predictor) #merge predictions with actual ratings\n",
    "#     print(f\"Ran for {time.time()-t1:.1f}s\")\n",
    "\n",
    "#     return predictions\n",
    "\n",
    "# #TODO more feature engineering for advisor 0 and 2\n",
    "# FE_fns = [append_district_counts,\n",
    "#           append_district_counts,\n",
    "#           append_district_counts,\n",
    "#           append_district_counts]\n",
    "\n",
    "# # advisor_train(grids, ratings, 0)\n",
    "# # advisor_predict(grids[0:2000], ratings[0:2000], 1)\n",
    "\n",
    "# all_predictions = []\n",
    "# t0 = time.time()\n",
    "# for i in range(0,4):\n",
    "#     advisor_predictions = advisor_predict(grids[:], ratings[:], i)\n",
    "#     all_predictions.append(advisor_predictions)\n",
    "# print(f'Predictions complete. Time required: {time.time()-t0:.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251022_050412\"\n",
      "Preset alias specified: 'medium' maps to 'medium_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.0\n",
      "Operating System:   Darwin\n",
      "Platform Machine:   arm64\n",
      "Platform Version:   Darwin Kernel Version 25.0.0: Wed Sep 17 21:41:50 PDT 2025; root:xnu-12377.1.9~141/RELEASE_ARM64_T6030\n",
      "CPU Count:          12\n",
      "Memory Avail:       14.98 GB / 36.00 GB (41.6%)\n",
      "Disk Space Avail:   224.79 GB / 460.43 GB (48.8%)\n",
      "===================================================\n",
      "Presets specified: ['medium']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"/Users/gtadams/Code/LGO/2.156/2155-Challenge-Problem-2/AutogluonModels/ag-20251022_050412\"\n",
      "Train Data Rows:    5000\n",
      "Train Data Columns: 88\n",
      "Label Column:       label\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    15345.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.68 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 88 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 88 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t88 features in original data used to generate 88 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 1.68 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.23s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'r2'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 4500, Val Rows: 500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.77s of the 59.77s of remaining time.\n",
      "\tFitting with cpus=12, gpus=0, mem=0.0/15.0 GB\n",
      "\t0.7198\t = Validation score   (r2)\n",
      "\t3.89s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 55.87s of the 55.87s of remaining time.\n",
      "\tFitting with cpus=12, gpus=0, mem=0.0/15.1 GB\n",
      "\t0.7233\t = Validation score   (r2)\n",
      "\t2.72s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 53.14s of the 53.14s of remaining time.\n",
      "\tFitting with cpus=12, gpus=0\n",
      "\t0.6853\t = Validation score   (r2)\n",
      "\t2.35s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 50.70s of the 50.70s of remaining time.\n",
      "\tFitting with cpus=12, gpus=0\n",
      "\t0.736\t = Validation score   (r2)\n",
      "\t3.27s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 47.42s of the 47.42s of remaining time.\n",
      "\tFitting with cpus=12, gpus=0\n",
      "\t0.6659\t = Validation score   (r2)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 46.17s of the 46.17s of remaining time.\n",
      "\tFitting with cpus=12, gpus=0, mem=0.0/15.0 GB\n",
      "No improvement since epoch 7: early stopping\n",
      "\t0.6871\t = Validation score   (r2)\n",
      "\t1.59s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 44.56s of the 44.56s of remaining time.\n",
      "\tFitting with cpus=12, gpus=0\n",
      "\t0.7194\t = Validation score   (r2)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 42.73s of the 42.73s of remaining time.\n",
      "\tFitting with cpus=12, gpus=0, mem=0.0/15.0 GB\n",
      "/Users/gtadams/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:975: FutureWarning: The parameter `force_int_remainder_cols` is deprecated and will be removed in 1.9. It has no effect. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "\t0.702\t = Validation score   (r2)\n",
      "\t2.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 39.73s of the 39.73s of remaining time.\n",
      "\tFitting with cpus=12, gpus=0, mem=0.1/14.9 GB\n",
      "\t0.7158\t = Validation score   (r2)\n",
      "\t12.74s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.77s of the 26.96s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.478, 'NeuralNetTorch': 0.261, 'XGBoost': 0.217, 'LightGBM': 0.043}\n",
      "\t0.7439\t = Validation score   (r2)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 33.07s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 40896.9 rows/s (500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/Users/gtadams/Code/LGO/2.156/2155-Challenge-Problem-2/AutogluonModels/ag-20251022_050412\")\n",
      "Deleting model LightGBMXT. All files under /Users/gtadams/Code/LGO/2.156/2155-Challenge-Problem-2/AutogluonModels/ag-20251022_050412/models/LightGBMXT will be removed.\n",
      "Deleting model RandomForestMSE. All files under /Users/gtadams/Code/LGO/2.156/2155-Challenge-Problem-2/AutogluonModels/ag-20251022_050412/models/RandomForestMSE will be removed.\n",
      "Deleting model ExtraTreesMSE. All files under /Users/gtadams/Code/LGO/2.156/2155-Challenge-Problem-2/AutogluonModels/ag-20251022_050412/models/ExtraTreesMSE will be removed.\n",
      "Deleting model NeuralNetFastAI. All files under /Users/gtadams/Code/LGO/2.156/2155-Challenge-Problem-2/AutogluonModels/ag-20251022_050412/models/NeuralNetFastAI will be removed.\n",
      "Deleting model LightGBMLarge. All files under /Users/gtadams/Code/LGO/2.156/2155-Challenge-Problem-2/AutogluonModels/ag-20251022_050412/models/LightGBMLarge will be removed.\n",
      "These features in provided data are not utilized by the predictor and will be ignored: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]\n",
      "Computing feature importance via permutation shuffling for 0 features using 2000 rows with 10 shuffle sets... Time limit: 30s...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"88 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 88 missing columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87'] | 0 available columns: []\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/features/generators/abstract.py:356\u001b[39m, in \u001b[36mAbstractFeatureGenerator.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(X.columns) != \u001b[38;5;28mself\u001b[39m.features_in:\n\u001b[32m    354\u001b[39m         \u001b[38;5;66;03m# It comes at a cost when making a copy of the DataFrame,\u001b[39;00m\n\u001b[32m    355\u001b[39m         \u001b[38;5;66;03m# therefore, try avoid copying by checking the expected features first.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m         X = \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures_in\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\\n       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',\\n       '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36',\\n       '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48',\\n       '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60',\\n       '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72',\\n       '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84',\\n       '85', '86', '87'],\\n      dtype='object')] are in the [columns]\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 172\u001b[39m\n\u001b[32m    170\u001b[39m all_predictions = []\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m advisor \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m     \u001b[43madvisor_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvisor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# advisor_predictions = advisor_predict(grids[:], advisor)\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# all_predictions.append(advisor_predictions)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[95]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36madvisor_train\u001b[39m\u001b[34m(grids, ratings, advisor)\u001b[39m\n\u001b[32m    132\u001b[39m predictor.delete_models(models_to_keep=\u001b[33m'\u001b[39m\u001b[33mbest\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Compute and display permutation feature importance on training data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m fi = \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m fi_top = fi.sort_values(\u001b[33m'\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m'\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m).head(\u001b[32m25\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTop feature importances:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/tabular/predictor/predictor.py:3490\u001b[39m, in \u001b[36mTabularPredictor.feature_importance\u001b[39m\u001b[34m(self, data, model, features, feature_stage, subsample_size, time_limit, num_shuffle_sets, include_confidence_band, confidence_level, silent)\u001b[39m\n\u001b[32m   3487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_shuffle_sets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3488\u001b[39m     num_shuffle_sets = \u001b[32m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m5\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3490\u001b[39m fi_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_learner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_shuffle_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_shuffle_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_confidence_band:\n\u001b[32m   3502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m confidence_level <= \u001b[32m0.5\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m confidence_level >= \u001b[32m1.0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/tabular/learner/abstract_learner.py:1010\u001b[39m, in \u001b[36mAbstractTabularLearner.get_feature_importance\u001b[39m\u001b[34m(self, model, X, y, features, feature_stage, subsample_size, silent, **kwargs)\u001b[39m\n\u001b[32m   1007\u001b[39m         X = X.drop(columns=unused_features)\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feature_stage == \u001b[33m\"\u001b[39m\u001b[33moriginal\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_feature_importance_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_func\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m     X = \u001b[38;5;28mself\u001b[39m.transform_features(X)\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py:3522\u001b[39m, in \u001b[36mAbstractTabularTrainer._get_feature_importance_raw\u001b[39m\u001b[34m(self, X, y, model, eval_metric, **kwargs)\u001b[39m\n\u001b[32m   3520\u001b[39m model: AbstractModel = \u001b[38;5;28mself\u001b[39m.load_model(model)\n\u001b[32m   3521\u001b[39m predict_func_kwargs = \u001b[38;5;28mdict\u001b[39m(model=model)\n\u001b[32m-> \u001b[39m\u001b[32m3522\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_permutation_feature_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3524\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_func_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_func_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3527\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantile_levels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantile_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3529\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3530\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/core/utils/utils.py:983\u001b[39m, in \u001b[36mcompute_permutation_feature_importance\u001b[39m\u001b[34m(X, y, predict_func, eval_metric, features, subsample_size, num_shuffle_sets, predict_func_kwargs, transform_func, transform_func_kwargs, time_limit, silent, log_prefix, importance_as_list, random_state, **kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subsample \u001b[38;5;129;01mor\u001b[39;00m shuffle_repeat == \u001b[32m0\u001b[39m:\n\u001b[32m    982\u001b[39m     time_start_score = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     X_transformed = X \u001b[38;5;28;01mif\u001b[39;00m transform_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtransform_func_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m     y_pred = predict_func(X_transformed, **predict_func_kwargs)\n\u001b[32m    985\u001b[39m     score_baseline = eval_metric(y, y_pred, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/tabular/learner/abstract_learner.py:488\u001b[39m, in \u001b[36mAbstractTabularLearner.transform_features\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature_generator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.feature_generators:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         X = \u001b[43mfeature_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/features/generators/abstract.py:362\u001b[39m, in \u001b[36mAbstractFeatureGenerator.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m X.columns:\n\u001b[32m    361\u001b[39m             missing_cols.append(col)\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m required columns are missing from the provided dataset to transform using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m missing columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X.columns))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m available columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(X.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m     )\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pre_astype_generator:\n\u001b[32m    368\u001b[39m     X = \u001b[38;5;28mself\u001b[39m._pre_astype_generator.transform(X)\n",
      "\u001b[31mKeyError\u001b[39m: \"88 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 88 missing columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87'] | 0 available columns: []\""
     ]
    }
   ],
   "source": [
    "## FEATURE ENGINEERING ##\n",
    "\n",
    "from scipy import ndimage\n",
    "\n",
    "def interface_count(grid, a, b):\n",
    "    # 4-neighbor interfaces between labels a and b\n",
    "    g = grid\n",
    "    right = (g[:, :-1] == a) & (g[:, 1:] == b)\n",
    "    right |= (g[:, :-1] == b) & (g[:, 1:] == a)\n",
    "    down = (g[:-1, :] == a) & (g[1:, :] == b)\n",
    "    down |= (g[:-1, :] == b) & (g[1:, :] == a)\n",
    "    return right.sum() + down.sum()\n",
    "\n",
    "def connectivity_features(grid, label):\n",
    "    mask = (grid == label).astype(np.uint8)\n",
    "    if mask.sum() == 0:\n",
    "        return 0, 0, 0.0\n",
    "    struct = np.array([[0,1,0],[1,1,1],[0,1,0]], dtype=np.uint8)  # 4-neighborhood\n",
    "    comp, n = ndimage.label(mask, structure=struct)\n",
    "    sizes = np.bincount(comp.ravel())[1:]\n",
    "    return n, sizes.max(), sizes.mean()\n",
    "\n",
    "def symmetry_scores(grid):\n",
    "    h = 1.0 - (np.mean(grid == np.fliplr(grid)))\n",
    "    v = 1.0 - (np.mean(grid == np.flipud(grid)))\n",
    "    return h, v\n",
    "\n",
    "def cooccurrence_4n(grid, n_labels=5):\n",
    "    # counts of adjacent pairs (unordered) across 4-neighbors\n",
    "    counts = np.zeros((n_labels, n_labels), dtype=np.int32)\n",
    "    # right neighbors\n",
    "    a, b = grid[:, :-1], grid[:, 1:]\n",
    "    for i in range(n_labels):\n",
    "        for j in range(i, n_labels):\n",
    "            c = ((a == i) & (b == j)) | ((a == j) & (b == i))\n",
    "            counts[i, j] += c.sum()\n",
    "            if i != j:\n",
    "                counts[j, i] = counts[i, j]\n",
    "    # down neighbors\n",
    "    a, b = grid[:-1, :], grid[1:, :]\n",
    "    for i in range(n_labels):\n",
    "        for j in range(i, n_labels):\n",
    "            c = ((a == i) & (b == j)) | ((a == j) & (b == i))\n",
    "            counts[i, j] += c.sum()\n",
    "            if i != j:\n",
    "                counts[j, i] = counts[i, j]\n",
    "    return counts\n",
    "\n",
    "def feature_eng_adv(grids):\n",
    "    # grids: (N, 7, 7) int\n",
    "    N = grids.shape[0]\n",
    "    feats = []\n",
    "    for g in grids:\n",
    "        counts = [(g == k).sum() for k in range(5)]\n",
    "        props = [c / 49.0 for c in counts]\n",
    "\n",
    "        # interfaces\n",
    "        iface = {(i, j): interface_count(g, i, j) for i in range(5) for j in range(i+1, 5)}\n",
    "        total_iface = sum(iface.values())\n",
    "\n",
    "        # connectivity for labels 0 and 3 (expand as needed)\n",
    "        n0, max0, mean0 = connectivity_features(g, 0)\n",
    "        n3, max3, mean3 = connectivity_features(g, 3)\n",
    "\n",
    "        # symmetry\n",
    "        sym_h, sym_v = symmetry_scores(g)\n",
    "\n",
    "        # center vs border for label 0\n",
    "        center_mask = np.zeros_like(g, dtype=bool)\n",
    "        center_mask[2:5, 2:5] = True\n",
    "        center0 = (g[center_mask] == 0).sum()\n",
    "        border0 = (g[~center_mask] == 0).sum()\n",
    "\n",
    "        # interactions\n",
    "        prop_pairs = [props[i]*props[j] for i in range(5) for j in range(i+1, 5)]\n",
    "        sdiv = lambda a,b: a / (b + 1e-6)\n",
    "        largest_ratio_0 = sdiv(max0, counts[0])\n",
    "        frag_0 = sdiv(n0, counts[0])\n",
    "        center_ratio_0 = sdiv(center0, center0 + border0)\n",
    "        iface_density = sdiv(total_iface, 84.0)  # total 4-neighbor edges in 7x7\n",
    "        iface_03_norm = sdiv(iface[(0,3)], counts[0] + counts[3])\n",
    "        sym_h_p0 = sym_h * props[0]\n",
    "        sym_v_p3 = sym_v * props[3]\n",
    "\n",
    "        feats.append([\n",
    "            *counts, *props,\n",
    "            iface[(0,3)], total_iface,\n",
    "            n0, max0, mean0, n3, max3, mean3,\n",
    "            sym_h, sym_v,\n",
    "            center0, border0,\n",
    "            *prop_pairs,\n",
    "            largest_ratio_0, frag_0, center_ratio_0,\n",
    "            iface_density, iface_03_norm,\n",
    "            sym_h_p0, sym_v_p3\n",
    "        ])\n",
    "\n",
    "    features = np.asarray(feats, dtype=np.float32)\n",
    "    grids_flat = grids.reshape(-1, 49) # flatten the grids\n",
    "    grids_out = np.hstack([grids_flat, features]) #stack the features horizontally with the flattened grids\n",
    "    return grids_out.astype(np.float32)\n",
    "    \n",
    "\n",
    "def append_district_counts(grids): #performs the feature engineering to add district counts\n",
    "    grids_flat = grids.reshape(-1, 49) #first flatten the grids\n",
    "\n",
    "    counts = [np.sum(grids_flat==0, axis=1),\n",
    "              np.sum(grids_flat==1, axis=1),\n",
    "              np.sum(grids_flat==2, axis=1),\n",
    "              np.sum(grids_flat==3, axis=1),\n",
    "              np.sum(grids_flat==4, axis=1)] #list of 5 length n_grids arrays containing counts of each district\n",
    "    features = np.stack(counts).T #stack and transpose counts to get n_grids x 5 array\n",
    "    grids_out = np.hstack([grids_flat, features]) #stack the features horizontally with the flattened grids\n",
    "    return grids_out.astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "## FITTING REGRESSORS ##\n",
    "    \n",
    "def advisor_train(grids, ratings, advisor):\n",
    "    FE_fn = FE_fns[advisor]\n",
    "    grids_train, ratings_train = select_rated_subset(grids, ratings[:,advisor]) #gets subset of the dataset rated by advisor\n",
    "    grids_fa = pd.DataFrame(FE_fn(grids_train)).astype(np.float32)\n",
    "\n",
    "    all_train = grids_fa.copy().astype(np.float32)\n",
    "    all_train[\"label\"] = ratings_train\n",
    "\n",
    "    predictor = TabularPredictor(label='label',\n",
    "                                 problem_type='regression',\n",
    "                                 verbosity=2,\n",
    "                                 eval_metric='r2')\n",
    "    predictor.fit(all_train, presets='medium', time_limit=60)\n",
    "    predictor.delete_models(models_to_keep='best')\n",
    "\n",
    "    # Compute and display permutation feature importance on training data (NOT WORKING)\n",
    "    # fi = predictor.feature_importance(all_train, subsample_size=2000, time_limit=30)\n",
    "    # fi_top = fi.sort_values('importance', ascending=False).head(25)\n",
    "    # print(\"Top feature importances:\")\n",
    "    # print(fi_top[['importance', 'stddev', 'p_value']])\n",
    "    # fi.to_csv(predictor.path + f\"/feature_importance_advisor{advisor}.csv\")\n",
    "\n",
    "    return predictor\n",
    "\n",
    "\n",
    "def advisor_predict(grids, advisor):\n",
    "    print(f\"Predicting Advisor {advisor}...\")\n",
    "    FE_fn = FE_fns[advisor]\n",
    "    predictor = TabularPredictor.load(\"AutogluonModels/\"+models[advisor])\n",
    "    grids_FE = pd.DataFrame(FE_fn(grids)).astype(np.float32) #feature engineering on full dataset\n",
    "    t0 = time.time()\n",
    "    predictions = predictor.predict(grids_FE[:2000]) #predict on full dataset\n",
    "    t1 = time.time()\n",
    "    est_run_time = (t1-t0)*len(grids)/2000\n",
    "    # print(f\"Estimate run time: {est_run_time:.1f}s\")\n",
    "    predictions = pd.concat([predictions, predictor.predict(grids_FE[2000:])]) #predict on full dataset\n",
    "    print(f\"Ran for {time.time()-t1:.1f}s\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "models = [\"ag-20251022_040544\",\n",
    "          \"ag-20251021_231536\",\n",
    "          \"ag-20251022_044013\",\n",
    "          \"ag-20251021_231705\",]\n",
    "\n",
    "#TODO more feature engineering for advisor 0 and 2\n",
    "FE_fns = [feature_eng_adv,\n",
    "          append_district_counts,\n",
    "          feature_eng_adv,\n",
    "          append_district_counts]\n",
    "\n",
    "all_predictions = []\n",
    "for advisor in [0, 2]:\n",
    "    advisor_train(grids, ratings, advisor)\n",
    "    # advisor_predictions = advisor_predict(grids[:], advisor)\n",
    "    # all_predictions.append(advisor_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes about models (20251021)\n",
    "\n",
    "| Advisor | Model number | OSR2 | Prediction Time |\n",
    "| :--- | :---: | ---: | ---: | \n",
    "| 0 | ag-20251022_040544 | 0.7442 | 7.2s |\n",
    "| 1 | ag-20251021_231536 | 0.9161 | 0.5s |\n",
    "| 2 | ag-20251022_044013 | 0.4718 | 3.3s |\n",
    "| 3 | ag-20251021_231705 | 0.9631 | 2.5s |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Leaderboard for ag-20251021_231536 (AutogluonModels/ag-20251021_231536)\n",
      "================================================================================\n",
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                 model  score_val eval_metric  pred_time_val   fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0             CatBoost   0.916089          r2       0.001515  32.244943                0.001515          32.244943            1       True          1\n",
      "1  WeightedEnsemble_L2   0.916089          r2       0.001725  32.264606                0.000210           0.019663            2       True          2\n",
      "Number of models trained: 2\n",
      "Types of models trained:\n",
      "{'WeightedEnsembleModel', 'CatBoostModel'}\n",
      "Bagging used: False \n",
      "Multi-layer stack-ensembling used: False \n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', []) : 54 | ['0', '1', '2', '3', '4', ...]\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gtadams/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/core/utils/plots.py:169: UserWarning: AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"\n",
      "  warnings.warn('AutoGluon summary plots cannot be created because bokeh is not installed. To see plots, please do: \"pip install bokeh==2.0.1\"')\n",
      "These features in provided data are not utilized by the predictor and will be ignored: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87]\n",
      "Computing feature importance via permutation shuffling for 0 features using 5000 rows with 5 shuffle sets...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"54 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 54 missing columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53'] | 0 available columns: []\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/features/generators/abstract.py:356\u001b[39m, in \u001b[36mAbstractFeatureGenerator.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    353\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(X.columns) != \u001b[38;5;28mself\u001b[39m.features_in:\n\u001b[32m    354\u001b[39m         \u001b[38;5;66;03m# It comes at a cost when making a copy of the DataFrame,\u001b[39;00m\n\u001b[32m    355\u001b[39m         \u001b[38;5;66;03m# therefore, try avoid copying by checking the expected features first.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m         X = \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures_in\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:6261\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\\n       '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24',\\n       '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36',\\n       '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48',\\n       '49', '50', '51', '52', '53'],\\n      dtype='object')] are in the [columns]\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[90]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m all_train = grids_fa.copy().astype(np.float32)\n\u001b[32m     15\u001b[39m all_train[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m] = ratings_train\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mpredictor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/tabular/predictor/predictor.py:3490\u001b[39m, in \u001b[36mTabularPredictor.feature_importance\u001b[39m\u001b[34m(self, data, model, features, feature_stage, subsample_size, time_limit, num_shuffle_sets, include_confidence_band, confidence_level, silent)\u001b[39m\n\u001b[32m   3487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_shuffle_sets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3488\u001b[39m     num_shuffle_sets = \u001b[32m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m time_limit \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m5\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m3490\u001b[39m fi_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_learner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeature_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeature_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtime_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_shuffle_sets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_shuffle_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m include_confidence_band:\n\u001b[32m   3502\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m confidence_level <= \u001b[32m0.5\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m confidence_level >= \u001b[32m1.0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/tabular/learner/abstract_learner.py:1010\u001b[39m, in \u001b[36mAbstractTabularLearner.get_feature_importance\u001b[39m\u001b[34m(self, model, X, y, features, feature_stage, subsample_size, silent, **kwargs)\u001b[39m\n\u001b[32m   1007\u001b[39m         X = X.drop(columns=unused_features)\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feature_stage == \u001b[33m\"\u001b[39m\u001b[33moriginal\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_feature_importance_raw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform_func\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1013\u001b[39m     X = \u001b[38;5;28mself\u001b[39m.transform_features(X)\n\u001b[32m   1014\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/tabular/trainer/abstract_trainer.py:3522\u001b[39m, in \u001b[36mAbstractTabularTrainer._get_feature_importance_raw\u001b[39m\u001b[34m(self, X, y, model, eval_metric, **kwargs)\u001b[39m\n\u001b[32m   3520\u001b[39m model: AbstractModel = \u001b[38;5;28mself\u001b[39m.load_model(model)\n\u001b[32m   3521\u001b[39m predict_func_kwargs = \u001b[38;5;28mdict\u001b[39m(model=model)\n\u001b[32m-> \u001b[39m\u001b[32m3522\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_permutation_feature_importance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3523\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3524\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3525\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3526\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict_func_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict_func_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3527\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3528\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantile_levels\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantile_levels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3529\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3530\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/core/utils/utils.py:983\u001b[39m, in \u001b[36mcompute_permutation_feature_importance\u001b[39m\u001b[34m(X, y, predict_func, eval_metric, features, subsample_size, num_shuffle_sets, predict_func_kwargs, transform_func, transform_func_kwargs, time_limit, silent, log_prefix, importance_as_list, random_state, **kwargs)\u001b[39m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subsample \u001b[38;5;129;01mor\u001b[39;00m shuffle_repeat == \u001b[32m0\u001b[39m:\n\u001b[32m    982\u001b[39m     time_start_score = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m983\u001b[39m     X_transformed = X \u001b[38;5;28;01mif\u001b[39;00m transform_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtransform_func_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m     y_pred = predict_func(X_transformed, **predict_func_kwargs)\n\u001b[32m    985\u001b[39m     score_baseline = eval_metric(y, y_pred, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/tabular/learner/abstract_learner.py:488\u001b[39m, in \u001b[36mAbstractTabularLearner.transform_features\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    487\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feature_generator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.feature_generators:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m         X = \u001b[43mfeature_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/autogluon/features/generators/abstract.py:362\u001b[39m, in \u001b[36mAbstractFeatureGenerator.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    360\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m X.columns:\n\u001b[32m    361\u001b[39m             missing_cols.append(col)\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    363\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m required columns are missing from the provided dataset to transform using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    364\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(missing_cols)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m missing columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(X.columns))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m available columns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(X.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    366\u001b[39m     )\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pre_astype_generator:\n\u001b[32m    368\u001b[39m     X = \u001b[38;5;28mself\u001b[39m._pre_astype_generator.transform(X)\n",
      "\u001b[31mKeyError\u001b[39m: \"54 required columns are missing from the provided dataset to transform using AutoMLPipelineFeatureGenerator. 54 missing columns: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53'] | 0 available columns: []\""
     ]
    }
   ],
   "source": [
    "## Autogluon Model leaderboard\n",
    "model_root = \"AutogluonModels\"\n",
    "model_dirs = sorted([d for d in os.listdir(model_root) if os.path.isdir(os.path.join(model_root, d))])\n",
    "for d in model_dirs:\n",
    "    path = os.path.join(model_root, d)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Leaderboard for {d} ({path})\")\n",
    "    print(\"=\"*80)\n",
    "    predictor = TabularPredictor.load(path)\n",
    "    summary = predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Best Grids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# TODO pick new grids if these are invalid, find a better way to pick grids\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m final_prediction_array = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_predictions\u001b[49m\u001b[43m)\u001b[49m.T \u001b[38;5;66;03m#stack the predictions\u001b[39;00m\n\u001b[32m      3\u001b[39m min_predictions = np.min(final_prediction_array, axis=\u001b[32m1\u001b[39m) \u001b[38;5;66;03m#minimum advisor score (as predicted)\u001b[39;00m\n\u001b[32m      4\u001b[39m valid_grids = np.sum(min_predictions>\u001b[32m0.75\u001b[39m) \u001b[38;5;66;03m#number of valid grids (as predicted)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/LGO/2.156/2155-Challenge-Problem-2/.venv/lib/python3.12/site-packages/numpy/_core/shape_base.py:444\u001b[39m, in \u001b[36mstack\u001b[39m\u001b[34m(arrays, axis, out, dtype, casting)\u001b[39m\n\u001b[32m    442\u001b[39m arrays = [asanyarray(arr) \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m arrays:\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mneed at least one array to stack\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    446\u001b[39m shapes = {arr.shape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[32m    447\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) != \u001b[32m1\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "# TODO pick new grids if these are invalid, find a better way to pick grids\n",
    "final_prediction_array = np.stack(all_predictions).T #stack the predictions\n",
    "min_predictions = np.min(final_prediction_array, axis=1) #minimum advisor score (as predicted)\n",
    "valid_grids = np.sum(min_predictions>0.75) #number of valid grids (as predicted)\n",
    "top_100_indices = np.argpartition(min_predictions, -100)[-100:] #indices of top 100 designs (as sorted by minimum advisor score)\n",
    "top_100_grids = grids[top_100_indices] #get the top 100 grids\n",
    "\n",
    "print(f\"Number of valid grids (as predicted): {valid_grids}\")\n",
    "if valid_grids >= 100:\n",
    "    score = diversity_score(top_100_grids)\n",
    "    print(f\"Current diversity score: {score:.4f}\")\n",
    "\n",
    "    best_submission = np.load(\"submission.npy\")\n",
    "    loaded_score = diversity_score(best_submission)\n",
    "    if score > loaded_score:\n",
    "        final_submission = grids[top_100_indices].astype(int)\n",
    "        # id = np.random.randint(1e8, 1e9-1)\n",
    "        # np.save(f\"{id}.npy\", final_submission) # uncomment this for real submission\n",
    "        np.save(f\"submission.npy\", final_submission) \n",
    "        print(f\"Score improved! Previous best: {loaded_score:.4f}\")\n",
    "        print(\"Submission saved\")\n",
    "    elif score <= loaded_score:\n",
    "        print(f\"No improvement. Previous best: {loaded_score:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough valid grids found\")\n",
    "\n",
    "plot_ratings_histogram(final_prediction_array[top_100_indices], withmin=True) #plot histograms of top 100 designs\n",
    "plot_n_grids(top_100_grids[-7:])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
